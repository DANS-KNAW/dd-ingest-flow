{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dd-ingest-flow \u00b6 Ingests DANS deposit directories into Dataverse. SYNOPSIS \u00b6 # Start server dd-ingest-flow { server | check } # Client ingest-flow-* For ingest-flow-* commands see dans-datastation-tools . DESCRIPTION \u00b6 Summary \u00b6 The dd-ingest-flow service imports deposit directories into Dataverse. If successful, this will result in a new dataset in Dataverse or a new version of an existing dataset. The input deposit directories must be located in a directory on local disk storage known as an ingest area . Ingest areas \u00b6 An ingest area is a directory on local disk storage that is used by the service to receive deposits. It contains the following subdirectories: inbox - the directory under which all input deposits must be located outbox - a directory where the processed deposit are moved to (if successful to a subdirectory processed , otherwise to one of rejected or failed ) The service supports three ingest areas: import - for bulk import of deposits, triggered by a data manager; migration - for bulk import of datasets migrated from EASY ; auto-ingest - for continuous import of deposits offered through deposit service, such as dd-sword2 ; Processing of a deposit \u00b6 Order of deposit processing \u00b6 A deposit directory represents one dataset version. The version history of a datasets is represented by a sequence of deposit directories. When enqueuing deposits the program will first order them by the timestamp in the Created element in the contained bag's bag-info.txt file. Processing steps \u00b6 The processing of a deposit consists of the following steps: Check that the deposit is a valid deposit directory . Check that the bag in the deposit is a valid v1 DANS bag . Map the dataset level metadata to the metadata fields expected in the target Dataverse. If: deposit represents first version of a dataset: create a new dataset draft. deposit represents an update to an existing dataset: draft a new version . Publish the new dataset-version. Update-deposit \u00b6 When receiving a deposit that specifies a new version for an existing dataset (an update-deposit) the assumption is that the bag contains the metadata and file data that must be in the new version. This means: The metadata specified completely overwrites the metadata in the latest version. So, even if the client needs to change only one word, it must send all the existing metadata with only that particular word changed. Any metadata left out will be deleted in the new version. The files will replace the files in the latest version. So the files that are in the deposit are the ones that will be in the new version. If a file is to be deleted from the new version, it should simply be left out in the deposit. If a file is to remain unchanged in the new version, an exact copy of the current file must be sent. File path is key The local file path (in Dataverse terms: directoryLabel + name) is used as the key to determine what file in the latest published version, if any, is targetting. For example, to replace a published file with label foo.txt and directoryLabel my/special/folder , the bag must contain the new version at data/my/special/folder/foo.txt . (Note that the directoryLabel is the path relative to the bag's data/ folder.) Mapping to Dataverse dataset \u00b6 The mapping rules are documented in the spreadsheet DD Ingest Flow Mapping Rules . Access to the Google spreadsheet is granted on request to customers of DANS. The spreadsheet includes rules for: dataset level metadata dataset terms file level metadata and attributes (including setting an embargo) ARGUMENTS \u00b6 Server \u00b6 positional arguments: {server,check} available commands named arguments: -h, --help show this help message and exit -v, --version show the application version and exit Client \u00b6 The service has a RESTful API. In dans-datastation-tools commands to manage the service are available. These commands have names starting with ingest-flow- . INSTALLATION AND CONFIGURATION \u00b6 Currently this project is built as an RPM package for RHEL7/CentOS7 and later. The RPM will install the binaries to /opt/dans.knaw.nl/dd-ingest-flow and the configuration files to /etc/opt/dans.knaw.nl/dd-ingest-flow . The configuration options are documented by comments in the default configuration file config.yml . To install the module on systems that do not support RPM, you can copy and unarchive the tarball to the target host. You will have to take care of placing the files in the correct locations for your system yourself. For instructions on building the tarball, see next section. BUILDING FROM SOURCE \u00b6 Prerequisites: Java 11 or higher Maven 3.3.3 or higher RPM Steps: git clone https://github.com/DANS-KNAW/dd-ingest-flow.git cd dd-ingest-flow mvn clean install If the rpm executable is found at /usr/local/bin/rpm , the build profile that includes the RPM packaging will be activated. If rpm is available, but at a different path, then activate it by using Maven's -P switch: mvn -Pprm install . Alternatively, to build the tarball execute: mvn clean install assembly:single","title":"Manual"},{"location":"#dd-ingest-flow","text":"Ingests DANS deposit directories into Dataverse.","title":"dd-ingest-flow"},{"location":"#synopsis","text":"# Start server dd-ingest-flow { server | check } # Client ingest-flow-* For ingest-flow-* commands see dans-datastation-tools .","title":"SYNOPSIS"},{"location":"#description","text":"","title":"DESCRIPTION"},{"location":"#summary","text":"The dd-ingest-flow service imports deposit directories into Dataverse. If successful, this will result in a new dataset in Dataverse or a new version of an existing dataset. The input deposit directories must be located in a directory on local disk storage known as an ingest area .","title":"Summary"},{"location":"#ingest-areas","text":"An ingest area is a directory on local disk storage that is used by the service to receive deposits. It contains the following subdirectories: inbox - the directory under which all input deposits must be located outbox - a directory where the processed deposit are moved to (if successful to a subdirectory processed , otherwise to one of rejected or failed ) The service supports three ingest areas: import - for bulk import of deposits, triggered by a data manager; migration - for bulk import of datasets migrated from EASY ; auto-ingest - for continuous import of deposits offered through deposit service, such as dd-sword2 ;","title":"Ingest areas"},{"location":"#processing-of-a-deposit","text":"","title":"Processing of a deposit"},{"location":"#order-of-deposit-processing","text":"A deposit directory represents one dataset version. The version history of a datasets is represented by a sequence of deposit directories. When enqueuing deposits the program will first order them by the timestamp in the Created element in the contained bag's bag-info.txt file.","title":"Order of deposit processing"},{"location":"#processing-steps","text":"The processing of a deposit consists of the following steps: Check that the deposit is a valid deposit directory . Check that the bag in the deposit is a valid v1 DANS bag . Map the dataset level metadata to the metadata fields expected in the target Dataverse. If: deposit represents first version of a dataset: create a new dataset draft. deposit represents an update to an existing dataset: draft a new version . Publish the new dataset-version.","title":"Processing steps"},{"location":"#update-deposit","text":"When receiving a deposit that specifies a new version for an existing dataset (an update-deposit) the assumption is that the bag contains the metadata and file data that must be in the new version. This means: The metadata specified completely overwrites the metadata in the latest version. So, even if the client needs to change only one word, it must send all the existing metadata with only that particular word changed. Any metadata left out will be deleted in the new version. The files will replace the files in the latest version. So the files that are in the deposit are the ones that will be in the new version. If a file is to be deleted from the new version, it should simply be left out in the deposit. If a file is to remain unchanged in the new version, an exact copy of the current file must be sent. File path is key The local file path (in Dataverse terms: directoryLabel + name) is used as the key to determine what file in the latest published version, if any, is targetting. For example, to replace a published file with label foo.txt and directoryLabel my/special/folder , the bag must contain the new version at data/my/special/folder/foo.txt . (Note that the directoryLabel is the path relative to the bag's data/ folder.)","title":"Update-deposit"},{"location":"#mapping-to-dataverse-dataset","text":"The mapping rules are documented in the spreadsheet DD Ingest Flow Mapping Rules . Access to the Google spreadsheet is granted on request to customers of DANS. The spreadsheet includes rules for: dataset level metadata dataset terms file level metadata and attributes (including setting an embargo)","title":"Mapping to Dataverse dataset"},{"location":"#arguments","text":"","title":"ARGUMENTS"},{"location":"#server","text":"positional arguments: {server,check} available commands named arguments: -h, --help show this help message and exit -v, --version show the application version and exit","title":"Server"},{"location":"#client","text":"The service has a RESTful API. In dans-datastation-tools commands to manage the service are available. These commands have names starting with ingest-flow- .","title":"Client"},{"location":"#installation-and-configuration","text":"Currently this project is built as an RPM package for RHEL7/CentOS7 and later. The RPM will install the binaries to /opt/dans.knaw.nl/dd-ingest-flow and the configuration files to /etc/opt/dans.knaw.nl/dd-ingest-flow . The configuration options are documented by comments in the default configuration file config.yml . To install the module on systems that do not support RPM, you can copy and unarchive the tarball to the target host. You will have to take care of placing the files in the correct locations for your system yourself. For instructions on building the tarball, see next section.","title":"INSTALLATION AND CONFIGURATION"},{"location":"#building-from-source","text":"Prerequisites: Java 11 or higher Maven 3.3.3 or higher RPM Steps: git clone https://github.com/DANS-KNAW/dd-ingest-flow.git cd dd-ingest-flow mvn clean install If the rpm executable is found at /usr/local/bin/rpm , the build profile that includes the RPM packaging will be activated. If rpm is available, but at a different path, then activate it by using Maven's -P switch: mvn -Pprm install . Alternatively, to build the tarball execute: mvn clean install assembly:single","title":"BUILDING FROM SOURCE"},{"location":"arch/","text":"DANS Data Station Architecture \u00b6 This module is a component in the DANS Data Station Architecture .","title":"\u21d2 DANS Data Station Architecture"},{"location":"arch/#dans-data-station-architecture","text":"This module is a component in the DANS Data Station Architecture .","title":"DANS Data Station Architecture"},{"location":"dev/","text":"Development \u00b6 This page contains information for developers about how to contribute to this project. Set-up \u00b6 This project can be used in combination with dans-dev-tools . Before you can start it as a service some dependencies must first be started: HSQL database \u00b6 Open a separate terminal tab: start-hsqldb-server.sh Dataverse instance \u00b6 The service needs a Dataverse instance to talk to. For this you can use for example dev_archaeology (only accessible to DANS developers): start-preprovisioned-box.py -s After start-up: vagrant ssh curl -X PUT -d s3kretKey http://localhost:8080/api/admin/settings/:BlockedApiKey curl -X PUT -d unblock-key http://localhost:8080/api/admin/settings/:BlockedApiPolicy This is necessary to allow calls to admin API endpoints from outside the box. This will break access to the admin API from within the box. To roll back to the original situation: curl -X PUT -d localhost-only http://localhost:8080/api/admin/settings/:BlockedApiPolicy/?unblock-key=s3kretKey dd-validate-dans-bag \u00b6 dd-ingest-flow uses dd-validate-dans-bag to validate the bag in the deposit. The validation service must be run outside the vagrant box, because it needs disk access to the deposit. Open a separate terminal tab: start-env.sh # only first time Configure the correct API key in etc/config.yml , also set validation.baseFolder to the absolute path of the data directory of dd-ingest-flow . Now you can start the service: start-service.sh dd-ingest-flow \u00b6 Open both projects in separate terminal tabs do the following for each: start-env.sh # only first time Configure the correct API keys ( apiKey and unblockKey ) in etc/config.yml . Note the apiKey overrides per ingest area. Now you can start the service: start-service.sh Prepare and start a deposit \u00b6 Once the dependencies and services are started you can ingest a single deposit by moving (not copy) a deposit into data/auto-ingest/inbox or whatever directory is configured in dd-ingest-flow/etc/config.yml ingestFlow:autoIngest:inbox Note that a migration bag has more data than valid for this process. The validator will inform you about what to remove and how to fix the checksums. The dans-datastation-tools project has commands to copy/move your data into an ingest_area (auto-ingest/import/migration) require a user group deposits . When running locally you don't have such a group, so you can't use these commands. Make sure to have the following structure. dd-ingest-flow \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 auto-ingest \u2502 \u2502 \u251c\u2500\u2500 inbox \u2502 \u2502 \u2502 \u2514\u2500\u2500 <UUID> \u2502 \u2502 \u2502 \u251c\u2500\u2500 bag \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 * \u2502 \u2502 \u2502 \u2514\u2500\u2500 deposit.properties \u2502 \u2502 \u2514\u2500\u2500 out \u2502 \u2514\u2500\u2500 tmp Alternatively you can prepare batches in one of the other ingest areas and start as follows. Configure the ingest_flow section and dataverse section of .dans-datastation-tools.yml which is a copy of src/datastation/example-dans-datastation-tools.yml . service_baseurl should refer to localhost The ingest_areas should refer to the same folders as the ingestFlow section of dd-ingest-flow/etc/config.yml . Replace the default /var/opt/dans.knaw.nl/tmp in the latter with data . Set the apiKey To repeat a test you'll need the dv-dataset-destroy script which needs safety_latch: OFF , the default is ON . Assuming dans-datastation-tools and dd-ingest-flow are in the same directory: cd ~/git/service/data-station/dans-datastation-tools poetry run ingest-flow start-migration -s ../dd-ingest-flow/data/migration/inbox/<SOME-DIR>/<UUID>","title":"Development"},{"location":"dev/#development","text":"This page contains information for developers about how to contribute to this project.","title":"Development"},{"location":"dev/#set-up","text":"This project can be used in combination with dans-dev-tools . Before you can start it as a service some dependencies must first be started:","title":"Set-up"},{"location":"dev/#hsql-database","text":"Open a separate terminal tab: start-hsqldb-server.sh","title":"HSQL database"},{"location":"dev/#dataverse-instance","text":"The service needs a Dataverse instance to talk to. For this you can use for example dev_archaeology (only accessible to DANS developers): start-preprovisioned-box.py -s After start-up: vagrant ssh curl -X PUT -d s3kretKey http://localhost:8080/api/admin/settings/:BlockedApiKey curl -X PUT -d unblock-key http://localhost:8080/api/admin/settings/:BlockedApiPolicy This is necessary to allow calls to admin API endpoints from outside the box. This will break access to the admin API from within the box. To roll back to the original situation: curl -X PUT -d localhost-only http://localhost:8080/api/admin/settings/:BlockedApiPolicy/?unblock-key=s3kretKey","title":"Dataverse instance"},{"location":"dev/#dd-validate-dans-bag","text":"dd-ingest-flow uses dd-validate-dans-bag to validate the bag in the deposit. The validation service must be run outside the vagrant box, because it needs disk access to the deposit. Open a separate terminal tab: start-env.sh # only first time Configure the correct API key in etc/config.yml , also set validation.baseFolder to the absolute path of the data directory of dd-ingest-flow . Now you can start the service: start-service.sh","title":"dd-validate-dans-bag"},{"location":"dev/#dd-ingest-flow","text":"Open both projects in separate terminal tabs do the following for each: start-env.sh # only first time Configure the correct API keys ( apiKey and unblockKey ) in etc/config.yml . Note the apiKey overrides per ingest area. Now you can start the service: start-service.sh","title":"dd-ingest-flow"},{"location":"dev/#prepare-and-start-a-deposit","text":"Once the dependencies and services are started you can ingest a single deposit by moving (not copy) a deposit into data/auto-ingest/inbox or whatever directory is configured in dd-ingest-flow/etc/config.yml ingestFlow:autoIngest:inbox Note that a migration bag has more data than valid for this process. The validator will inform you about what to remove and how to fix the checksums. The dans-datastation-tools project has commands to copy/move your data into an ingest_area (auto-ingest/import/migration) require a user group deposits . When running locally you don't have such a group, so you can't use these commands. Make sure to have the following structure. dd-ingest-flow \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 auto-ingest \u2502 \u2502 \u251c\u2500\u2500 inbox \u2502 \u2502 \u2502 \u2514\u2500\u2500 <UUID> \u2502 \u2502 \u2502 \u251c\u2500\u2500 bag \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 * \u2502 \u2502 \u2502 \u2514\u2500\u2500 deposit.properties \u2502 \u2502 \u2514\u2500\u2500 out \u2502 \u2514\u2500\u2500 tmp Alternatively you can prepare batches in one of the other ingest areas and start as follows. Configure the ingest_flow section and dataverse section of .dans-datastation-tools.yml which is a copy of src/datastation/example-dans-datastation-tools.yml . service_baseurl should refer to localhost The ingest_areas should refer to the same folders as the ingestFlow section of dd-ingest-flow/etc/config.yml . Replace the default /var/opt/dans.knaw.nl/tmp in the latter with data . Set the apiKey To repeat a test you'll need the dv-dataset-destroy script which needs safety_latch: OFF , the default is ON . Assuming dans-datastation-tools and dd-ingest-flow are in the same directory: cd ~/git/service/data-station/dans-datastation-tools poetry run ingest-flow start-migration -s ../dd-ingest-flow/data/migration/inbox/<SOME-DIR>/<UUID>","title":"Prepare and start a deposit"}]}